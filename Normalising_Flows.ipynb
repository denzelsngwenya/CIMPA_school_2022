{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c586c009",
   "metadata": {},
   "source": [
    "# Normalising flows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0feadf",
   "metadata": {},
   "source": [
    "To read more about normalising flows, you can check out [Towards Data Science](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b#:~:text=In%20simple%20words%2C%20normalizing%20flows,is%20not%20a%20reversible%20function.) there is a comprehensive summary of Normalising flows and the intuition behind normalising flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ebb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from torch.utils.data import DataLoader, Subset\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from tqdm import trange\n",
    "\n",
    "from IPython.display import Image\n",
    "from types import MethodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b28b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5601d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pinwheel(num_samples):\n",
    "    rng = np.random.RandomState()\n",
    "    radial_std = 0.3\n",
    "    tangential_std = 0.1\n",
    "    num_classes = 5\n",
    "    rate = 0.25\n",
    "    rads = np.linspace(0, 2*np.pi, num_classes, endpoint=False)\n",
    "\n",
    "    features = rng.randn(num_samples, 2) \\\n",
    "            * np.array([radial_std, tangential_std])\n",
    "    features[:, 0] += 1.\n",
    "    labels = rng.randint(0, num_classes, num_samples)\n",
    "\n",
    "    angles = rads[labels] + rate*np.exp(features[:, 0])\n",
    "    rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
    "    rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
    "\n",
    "    wheels = 2*rng.permutation(np.einsum(\"ti,tij->tj\", features, rotations))\n",
    "\n",
    "    return torch.tensor(wheels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18530eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = sample_pinwheel(5000)\n",
    "fig, ax = plt.subplots(1,2, figsize=(9,3), sharex=True, sharey=True)\n",
    "x_0 = pin[:, 0].numpy()\n",
    "x_1 = pin[:, 1].numpy()\n",
    "ax[0].scatter(x_0, x_1, alpha=.2)\n",
    "\n",
    "# This KDE plot function call constructs an estimate of the density from the sample\n",
    "sns.kdeplot(x=x_0, y=x_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Pinwheel scatterplot')\n",
    "ax[1].set_title('Pinwheel density estimate')\n",
    "\n",
    "for axi in ax:\n",
    "    axi.set_xlim((-4,4))\n",
    "    axi.set_ylim((-4,4))\n",
    "    axi.axes.xaxis.set_visible(False)\n",
    "    axi.axes.yaxis.set_visible(False)\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85b4bf",
   "metadata": {},
   "source": [
    "Each point in the pinwheel is randomly drawn independently and identically from the others, from an underlying distribution over $\\mathbb{R}^2$ specified implicitly by the code above.\n",
    "\n",
    "What we would now like to do is model this distribution.\n",
    "Modelling a distribution is usually desirable to enable us to perform two main tasks, namely:\n",
    "\n",
    "- calculate the density of an observation under the distribution; or\n",
    "- sample new observations from the distribution.\n",
    "\n",
    "In practice, we usually don't have access to the code/mechanism generating points: instead, we typically only have access to a sample of observations.\n",
    "\n",
    "This section will use normalizing flows to tackle this problem.\n",
    "Flows enable us to both evaluate the density function of a distribution, as well as sample points from it.\n",
    "The key assumption we will make is that we can _associate_ this complicated pinwheel distribution, which we know little about, with a _simple_ distribution that we understand well (i.e. we can sample from it, and evaluate its density).\n",
    "Once we understand this association, we can perform the tasks above for the complicated pinwheel distribution by relating it to the solutions to these tasks for the simple distribution.\n",
    "\n",
    "In practice, the simple distribution is often chosen to be a standard multivariate normal distribution; for technical reasons, this distribution must be the same dimensionality as the data we are to model.\n",
    "Since the pinwheel data has two dimensions, we will use a two-dimensional standard normal distribution as our simple distribution.\n",
    "\n",
    "More specifically, the association between the two distributions we are seeking in this case is a function $F : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ so that:\n",
    "\n",
    "- if $(x, y)$ is a point drawn from the simple (multivariate Gaussian) distribution, then $F(x, y)$ is a point drawn from the pinwheel distribution; and\n",
    "- $F(\\cdot)$ is bijective, i.e. every point in the range of $F$ (\"pinwheel space\") is uniquely associated with a corresponding point in the domain of $F$ (\"Gaussian space\") and vice versa.\n",
    "\n",
    "Because $F(\\cdot)$ is bijective, its inverse $F^{-1}(\\cdot)$ exists and if $(x, y)$ is a point drawn from the pinwheel distribution, then $F^{-1}(x, y)$ is a point drawn from the multivariate Gaussian.\n",
    "\n",
    "Neural networks allow us to fit complex functions; the flows we develop in this practical will enhance multi-layer perceptrons (MLPs) - a simple form of feedforward neural network - to ensure that the resulting function $F$ will be bijective.\n",
    "For a flow to be useful, we also need to be able to sample or perform density estimation with it, and these operations should be reasonably efficient.\n",
    "This further constrains the choice of functions we will fit with our neural networks, and the specific form of flows used in practice depend on various tradeoffs regarding the planned usage of the fitted model.\n",
    "\n",
    "The main tradeoffs involve:\n",
    "\n",
    "- how well the flow is able to model the true mapping between the base and true distribution (_expressivity_);\n",
    "- how tractable it is to calculate the flow and/or its inverse (although the flow is invertible, computing the inverse may not be tractable); and\n",
    "- how tractable it is to calculate the Jacobian of the flow.\n",
    "\n",
    "Computing the Jacobian of the flow is important because the change in density incurred by the transformation $F$ is tracked via the _change-of-variables_ formula.\n",
    "Consider a random variable $\\mathbf{z} = \\mathbf{z}_0$ with a standard Gaussian distribution, $\\mathbf{z}_0 \\sim p_0 = \\mathcal{N}(\\mathbf{0}, I)$, to which we apply an invertible transformation to obtain the random variable $\\mathbf{x} = F(\\mathbf{z_0})$. The density of the resulting $\\mathbf{x}$ is then given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(\\mathbf{x}) &= \\log p_0(\\mathbf{z}_0) + \\log \\left|J_{F^{-1}}(\\mathbf{x})\\right| \\qquad (\\mbox{change-of-variables})\\\\\n",
    "&=  \\log p_0(\\mathbf{z}_0) + \\log \\left|J_{F}(\\mathbf{z}_0)\\right|^{-1} \\quad (\\mbox{inverse function theorem}) \\\\\n",
    "&= \\log p_0(\\mathbf{z}_0) - \\log \\left|J_{F}(\\mathbf{z}_0)\\right|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $J_F(a)$ denotes the Jacobian of the transformation $F$ at $a$. (See equation 1.27 in section 1.2.1 of \\[[5](#References)\\], and for a discussion on the derivation see [here](https://stats.stackexchange.com/questions/239588/derivation-of-change-of-variables-of-a-probability-density-function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c8736",
   "metadata": {},
   "source": [
    "### Normalizing Flows with RealNVP affine coupling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fea18",
   "metadata": {},
   "source": [
    "The form of the change-of-variables formula makes it possible to compose multiple simpler bijective transformations in order to obtain an overall transformation that is complex enough to provide an accurate mapping between the base and target distribution. So, let $F$ consist of a chain of simpler transformations:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = F(\\mathbf{z}_0) = f_T \\circ \\ldots \\circ f_1(\\mathbf{z}_0).\n",
    "$$\n",
    "\n",
    "Writing $\\mathbf{z}_{t}$ for $f_t(\\mathbf{z}_{t-1})$, we have $\\mathbf{x} = \\mathbf{z}^T$.\n",
    "Then we have that $\n",
    "\\log p_T(\\mathbf{x}) = \\log p_0(\\mathbf{z}_0) - \\sum_{t=1}^T \\log \\left|J_{f_t}(\\mathbf{z}_{t-1})\\right|\n",
    "$.\n",
    "\n",
    "For this practical we will consider _coupling_ transformations as presented in the paper 'Density estimation using Real NVP' \\[[7](#References)\\].\n",
    "In each simple bijection $f_t(\\mathbf{z}_{t-1})$, one half of the input vector\n",
    "( $\\mathbf{z}_{t-1,{1:k}}$ where $k \\lt K$ ) remains unchanged, while the rest of the input\n",
    "( $\\mathbf{z}_{t, k+1:K}$) is transformed using a function that is simple to invert, but which depends on $\\mathbf{z}_{t-1,1:k}$ in a complex way.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_{t, 1:k} &= \\mathbf{z}_{t-1, 1:k} \\\\\n",
    "\\mathbf{z}_{t, k+1:K} &= \\mathbf{z}_{t-1, k+1:K} \\odot \\exp\\left(s_t(\\mathbf{z}_{t-1, 1:k})\\right) + m_t(\\mathbf{z}_{t-1, 1:k})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $s_t$ and $m_t$ are MLPs each outputting $K-k$-dimensional vectors encoding the complex dependency above for $f_t$: we perform an affine transformation on the second part of the input, with $m_t$ specifying the translation term and $s_t$ implicitly specifying a scaling transformation for each component.\n",
    "\n",
    "This type of transformation is known as an _affine coupling layer_. Note that the transformation remains invertible for arbitrarily complex functions $s_t$ and $m_t$: the inverse is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_{t-1, 1:k} &= \\mathbf{z}_{t, 1:k}  \\\\\n",
    "\\mathbf{z}_{t-1, k+1:K} &= \\left(\\mathbf{z}_{t, k+1:K} - m_t(\\mathbf{z}_{t, 1:k})\\right) \\odot \\exp\\left( -s_t(\\mathbf{z}_{t, 1:k})\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To allow all the variables to influence each other, we will need to change which subset of variables remains constant and which subset gets updated at each successive transformation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43049043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6467f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, dim, conditioner_hidden_dims, reverse=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = dim//2 # Use k = K//2 for coupling layer\n",
    "        self.block2 = dim - self.block1\n",
    "\n",
    "        self.reverse = reverse # Control which half is kept constant vs transformed\n",
    "        if not self.reverse:\n",
    "            # block1 remains constant\n",
    "            # block 2 depends on block 1\n",
    "            self.s_net = self.build_mlp(self.block1, conditioner_hidden_dims, self.block2)\n",
    "            self.m_net = self.build_mlp(self.block1, conditioner_hidden_dims, self.block2)\n",
    "        else:\n",
    "            # block 2 remains constant\n",
    "            # block 1 depends on block 2\n",
    "            self.s_net = self.build_mlp(self.block2, conditioner_hidden_dims, self.block1)\n",
    "            self.m_net = self.build_mlp(self.block2, conditioner_hidden_dims, self.block1)\n",
    "\n",
    "    def build_mlp(self, in_dim, hidden_dims, out_dim):\n",
    "        '''Construct a multilayer perceptron neural network with ReLU\n",
    "        activation functions.\n",
    "\n",
    "        Parameters:\n",
    "        * in-dim: input width (int)\n",
    "        * hidden_dims: hidden layer widths (list of ints)\n",
    "        * out_dim: output width (int)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    # Note that the forward function is automatically executed when you\n",
    "    # \"call\" the surrounding instance, and this is the typical way in\n",
    "    # which this function is executed as part of a forward pass in a\n",
    "    # network.  The \"call\" to the surrounding instance passed on any\n",
    "    # arguments provided to the forward function call.  For example, to run\n",
    "    # this forward function, if you have a variable cl that is an instance\n",
    "    # of CouplingLayer, you should generally run its forward method as:\n",
    "    #\n",
    "    # cl(z)\n",
    "    #\n",
    "    # and not\n",
    "    #\n",
    "    # cl.forward(z)\n",
    "    def forward(self, z):\n",
    "        '''Applies this coupling transformation to z, and returns the result as well as the \n",
    "        log-Jacobian-determinant of the transformation. '''\n",
    "        s, m = self.conditioner(z)\n",
    "        z = self.coupling_transform(z, s, m)\n",
    "        return z, s.sum(1)\n",
    "\n",
    "    def inverse(self, z):\n",
    "        '''Applies the inverse of this coupling transformation to z, and returns the \n",
    "        result well as the the log-Jacobian-determinant of the inverse transformation.\n",
    "\n",
    "        If flow.forward(z) transforms z to z1, then flow.inverse(z1) should transform\n",
    "        z1 to z.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def conditioner(self, z):\n",
    "        '''Apply and return the result of the conditioner functions s and m for the layer input z.'''\n",
    "        pass\n",
    "\n",
    "    def coupling_transform(self, z, s, m):\n",
    "        '''Applies the transformer of the coupling layer, with conditioner outputs s and m, to z.'''\n",
    "        if not self.reverse:\n",
    "            z1 = z[:, :self.block1]\n",
    "            z2 = torch.exp(s)*z[:, self.block1:] + m\n",
    "        else:\n",
    "            z1 = torch.exp(s)*z[:, :self.block1] + m\n",
    "            z2 = z[:, self.block1:]\n",
    "        return torch.cat((z1, z2), dim=1)\n",
    "\n",
    "    def coupling_transform_inverse(self, z, s, m):\n",
    "        '''Applies the inverse of the transformer of the coupling layer to z, assuming the conditioner\n",
    "        outputs used to generate z were s and m.'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdf3c4",
   "metadata": {},
   "source": [
    "As a starter we will implement the ```build_mlp``` method for you.\n",
    "\n",
    "The method creates a new multilayer perceptron (MLP), which could be trained and used on its own; however, we use this in the CouplingLayer constructor to generate the MLPs for $m_t$ and $s_t$.\n",
    "The parameters ```in_dim``` and ```out_dim``` are natural numbers specifying the number of nodes in the input and output layers of the MLP to be created.\n",
    "```hidden_dims``` is a list of natural numbers that specifies the number of nodes in each hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(self, in_dim, hidden_dims, out_dim):\n",
    "    layer_widths = [in_dim] + hidden_dims + [out_dim]\n",
    "    layers = []\n",
    "    for i in range(len(layer_widths)-1):\n",
    "        in_dim = layer_widths[i]\n",
    "        out_dim = layer_widths[i+1]\n",
    "        layers += [nn.Linear(in_dim, out_dim), nn.ReLU()]\n",
    "    layers.pop() # get rid of the last nn.ReLU()\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bf894",
   "metadata": {},
   "source": [
    "As a sanity check, let's check that this function produces sensible output, by checking whether the output of our MLP on valid input produces the correctly shaped output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851380e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch shape: (batch_size, feature_dim)\n",
    "random_batch = torch.randn(5, 9)\n",
    "\n",
    "in_dim = None # TODO: change None to correct value\n",
    "#BEGIN_REMOVE\n",
    "in_dim = 9\n",
    "#END_REMOVE\n",
    "hidden_dims = [3, 2, 5]\n",
    "out_dim = 4\n",
    "\n",
    "# Create the MLP\n",
    "test_mlp = build_mlp(None, in_dim, hidden_dims, out_dim)\n",
    "# Predict with the MLP by applying the forward function of test_mlp to random_batch.\n",
    "# Since test_mlp is of type nn.Sequential, the forward functions\n",
    "# of each layer are executed in turn, with the output of each used\n",
    "# as the input to the next.  This effectively composes the functions\n",
    "# in the layers.\n",
    "out_mlp = test_mlp(random_batch) \n",
    "batch_size, output_dim = out_mlp.shape\n",
    "\n",
    "if batch_size == 5 and output_dim == 4:\n",
    "    print('Great success!')\n",
    "else:\n",
    "    print('Failure :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d1c5f",
   "metadata": {},
   "source": [
    "The instruction in the next cell replaces the default `pass` implementation of `build_mlp` in the `CouplingLayer` class by the version we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the previously empty definition with our new definition.\n",
    "CouplingLayer.build_mlp = build_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a3f1b",
   "metadata": {},
   "source": [
    "Now that we've implemented the `build_mlp` function which `CouplingLayer`'s constructor required, we can create an instance of `CouplingLayer` for testing the rest of our method implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dim = 16\n",
    "test_conditioner_hidden_dims = [15, 20, 10]\n",
    "test_cl = CouplingLayer(dim=test_dim, conditioner_hidden_dims=test_conditioner_hidden_dims)\n",
    "test_z = torch.randn(1, test_dim) # random input to test functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31a3f8",
   "metadata": {},
   "source": [
    "In order to get the output of a `CouplingLayer`, we need to be able to apply the `forward` function.  This requires the implementation of the `conditioner` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditioner(self, z):\n",
    "    '''Apply and return the result of the conditioner functions s and m for the layer input z.'''\n",
    "    # TODO: Implement. Use self.reverse to identify the appropriate inputs to self.s_net and self.m_net,\n",
    "    # and then compute and return the outputs of these MLPs on these inputs.\n",
    "    #BEGIN_REMOVE\n",
    "    if not self.reverse:\n",
    "        input_ = z[:, :self.block1]\n",
    "    else:\n",
    "        input_ = z[:, self.block1:]\n",
    "    s = self.s_net(input_)\n",
    "    m = self.m_net(input_)\n",
    "    #END_REMOVE\n",
    "    return s, m\n",
    "\n",
    "# Associate the function above with the CouplingLayer class\n",
    "CouplingLayer.conditioner = conditioner\n",
    "\n",
    "# test CouplingLayer - output of the first 8 dimensions must stay constant, others should change\n",
    "out_cond, _ = test_cl(test_z)\n",
    "\n",
    "if torch.equal(test_z[0, :8], out_cond[0, :8]) and not torch.equal(test_z[0, 8:], out_cond[0, 8:]):\n",
    "    print('Great success!')\n",
    "else:\n",
    "    print('Failure :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8dc4c",
   "metadata": {},
   "source": [
    "The functionality defined so far is enough to use `CouplingLayers` in a normalizing flow that can be trained using the _reverse_ KL divergence and used for _sampling_ from the flow.  (In fact, we will return to this way of using the flow towards the end of the practical.)\n",
    "However, we are now going to implement functionality for inverting the layer to allow _for density estimation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coupling_transform_inverse(self, z, s, m):\n",
    "    # TODO: Implement.  z denotes the usual output of the layer, given the outputs\n",
    "    # of the s_net and m_net outputs were s and m respectively.  Return the input\n",
    "    # that would have generated z.  Remember to take self.reverse into account.\n",
    "    # The function torch.cat for concatenating tensors might be helpful.\n",
    "    #BEGIN_REMOVE\n",
    "    if not self.reverse:\n",
    "        z_inv_1 = z[:, :self.block1]\n",
    "        z_inv_2 = (z[:, self.block1:] - m)*torch.exp(-s)\n",
    "    else:\n",
    "        z_inv_1 = (z[:, :self.block1] - m)*torch.exp(-s)\n",
    "        z_inv_2 = z[:, self.block1:]\n",
    "    return torch.cat((z_inv_1, z_inv_2), dim=1)\n",
    "    #END_REMOVE\n",
    "\n",
    "CouplingLayer.coupling_transform_inverse = coupling_transform_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(self, z):\n",
    "    # TODO: Implement. z denotes the usual output of the layer. Determine\n",
    "    # what the outputs of the s_net and m_net would have been and use them with\n",
    "    # the function you implemented above.  Return the input that would have\n",
    "    # generated z, as well as the log-Jacobian-determinant of this inverse\n",
    "    # transformation. You may want to consult the implementation of forward().\n",
    "    #BEGIN_REMOVE\n",
    "    s, m = self.conditioner(z)\n",
    "    z = self.coupling_transform_inverse(z, s, m)\n",
    "    return z, -s.sum(1)\n",
    "    #END_REMOVE\n",
    "\n",
    "CouplingLayer.inverse = inverse\n",
    "\n",
    "# Testing inverse and Jacobian\n",
    "z_forward, j_forward = test_cl(test_z)\n",
    "z_forward_inverse, j_forward_inverse = test_cl.inverse(z_forward)\n",
    "\n",
    "if torch.norm(test_z - z_forward_inverse) < 1e-6:\n",
    "    print('inverse calculated correctly')\n",
    "else:\n",
    "    print('inverse calculated incorrectly')\n",
    "if torch.abs(j_forward + j_forward_inverse) < 1e-6:\n",
    "    print('Jacobian calculated correctly')\n",
    "else:\n",
    "    print('Jacobian calculated incorrectly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193d998",
   "metadata": {},
   "source": [
    "We now use the ```CouplingLayer``` module as a building block to construct our flow.\n",
    "\n",
    "The skeleton is again given below.\n",
    "You will have to complete the inverse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPFlow(nn.Module):\n",
    "    def __init__(self, dim, num_steps, conditioner_hidden_dims):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps # number of CouplingLayers - generally use at least 3\n",
    "\n",
    "        # Initialize coupling transformation steps\n",
    "        steps = []\n",
    "        reverse = False\n",
    "        for i in range(num_steps):\n",
    "            steps.append(CouplingLayer(dim, conditioner_hidden_dims, reverse))\n",
    "            reverse = not reverse # Change which half is kept unchanged in each layer\n",
    "        # We don't use nn.Sequential here, because we must use a custom forward() \n",
    "        self.steps = nn.ModuleList(steps)\n",
    "\n",
    "    def forward(self, z0):\n",
    "        '''Apply the flow transformation steps to the input z0. \n",
    "        Returns the final transformed variable z as well as\n",
    "        the log Jacobian term in the change-of-variables formula.\n",
    "        '''\n",
    "        j_total = 0.0\n",
    "        zT = z0\n",
    "        for step in self.steps:\n",
    "            zT, j = step(zT) # Call forward on the CouplingLayer step\n",
    "            j_total += j\n",
    "\n",
    "        return zT, j_total\n",
    "\n",
    "    def inverse(self, zT):\n",
    "        '''Apply the inverse flow transformation steps to the output zT. \n",
    "        Returns the initial input variable z0 as well as\n",
    "        the log Jacobian term for the inverse transformation\n",
    "        in the change-of-variables formula.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def loglik(self, x):\n",
    "        '''Return the log-density of x for this RealNVPFlow instance.'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa97b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(self, zT):\n",
    "    # TODO: Implement. You need to invert the flow transformation\n",
    "    # implemented in RealNVPFlow.forward().\n",
    "    #BEGIN_REMOVE\n",
    "    j_total = 0.0\n",
    "    z0 = zT\n",
    "    for step in reversed(self.steps):\n",
    "        z0, j = step.inverse(z0)\n",
    "        j_total += j\n",
    "    #END_REMOVE\n",
    "    return z0, j_total\n",
    "\n",
    "RealNVPFlow.inverse = inverse\n",
    "\n",
    "# testing\n",
    "test_flow = RealNVPFlow(test_dim, 2, test_conditioner_hidden_dims)\n",
    "\n",
    "z_forward, j_forward = test_flow(test_z)\n",
    "z_forward_inverse, j_forward_inverse = test_flow.inverse(z_forward)\n",
    "\n",
    "if torch.norm(test_z - z_forward_inverse) < 1e-6:\n",
    "    print('inverse calculated correctly')\n",
    "else:\n",
    "    print('inverse calculated incorrectly')\n",
    "\n",
    "if torch.norm(j_forward + j_forward_inverse) < 1e-6:\n",
    "    print('Jacobian calculated correctly')\n",
    "else:\n",
    "    print('Jacobian calculated incorrectly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60ad97",
   "metadata": {},
   "source": [
    "We will now train an instance of the `RealNVPFlow` class to model the pinwheel distribution.  We use 8 flow steps, and illustrate the flow by plotting a large number of samples at various stages of training.  While it is instructive to work through this code, the key takeaway is to see how the flow evolves over the course of training.  You may develop your insight by adjusting some of the parameters of the flow or training hyperparameters.\n",
    "\n",
    "In order to train successfully, we need to specify the loss function to be optimized by the training loop.  In this case, we have training data, so minimizing the forward KL divergence, or equivalently maximizing the (log-)likelihood is applicable.  Implement the `loglik` function in the cell below for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the mean log likelihood over a batch, x.\n",
    "def loglik(self, x):\n",
    "    # normal distribution object for calculating loss\n",
    "    p0 = MultivariateNormal(torch.zeros(x.shape[-1]), torch.eye(x.shape[-1]))\n",
    "    # TODO: Complete\n",
    "    #BEGIN_REMOVE\n",
    "    u, log_j = self.inverse(x)\n",
    "    #print(\"x\")\n",
    "    #print(x)\n",
    "    #print(\"u\")\n",
    "    #print(u)\n",
    "    #print(\"log_j\")\n",
    "    #print(log_j)\n",
    "    ll = (p0.log_prob(u) + log_j).mean()\n",
    "    #END_REMOVE\n",
    "    return ll\n",
    "\n",
    "RealNVPFlow.loglik = loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edf055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup plotting\n",
    "cmap = 'brg'\n",
    "alpha = .4\n",
    "num_plots = 4\n",
    "fig, ax = plt.subplots(1,2+num_plots, figsize=(18,3), sharex=True, sharey=True)\n",
    "\n",
    "# create flow model\n",
    "dim = 2\n",
    "num_steps = 8\n",
    "conditioner_hidden_dims = [20, 20, 20]\n",
    "flow = RealNVPFlow(dim, num_steps, conditioner_hidden_dims)\n",
    "\n",
    "# set training hyperparameters\n",
    "num_epochs = 10*2**num_plots\n",
    "batch_size = 2**7\n",
    "optim = torch.optim.Adam(flow.parameters())\n",
    "\n",
    "# sample training data\n",
    "num_batches = 10\n",
    "X_train = sample_pinwheel(num_batches*batch_size)\n",
    "\n",
    "# plot validation set and reference latents\n",
    "X_val = sample_pinwheel(5*batch_size)\n",
    "x_0, x_1 = X_val[:, 0], X_val[:, 1]\n",
    "ax[0].scatter(x_0, x_1, alpha=alpha, cmap=cmap)\n",
    "ax[0].set_title('Data')\n",
    "\n",
    "# produce and plot standard normal data\n",
    "Z = torch.randn(10*batch_size, 2)\n",
    "z_0, z_1 = Z[:, 0], Z[:, 1]\n",
    "sample_noise_colors = np.zeros(shape=len(z_1), dtype=int)\n",
    "cuts = [-np.inf, -1, 1]\n",
    "for n, (low, high) in enumerate(zip(cuts, cuts[1:])):\n",
    "    sample_noise_colors[(low <= z_1)&(z_1 <= high)] = n + 1\n",
    "ax[1].scatter(z_0, z_1, alpha=alpha, c=sample_noise_colors, cmap=cmap)\n",
    "ax[1].set_title(r'$p_0=\\mathcal{N}(\\mathbf{0},I)$')\n",
    "\n",
    "# training loop\n",
    "nf_val_loss = []\n",
    "plot_epochs = [num_epochs//(2**(num_plots - i - 1)) for i in range(num_plots)]\n",
    "epochs = trange(num_epochs)\n",
    "for epoch in epochs:\n",
    "    for i in range(num_batches):\n",
    "        # Process the training set by batches.  (Later, we will use an object called\n",
    "        # a DataLoader for managing this for us.)\n",
    "        x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # take step in direction of negative gradient of loss\n",
    "        loss = -flow.loglik(x)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # for tracking validation loss, and producing nice plots of training flows\n",
    "    if (epoch+1) in plot_epochs:\n",
    "        with torch.no_grad():\n",
    "            # track validation loss\n",
    "            loss = -flow.loglik(X_val)\n",
    "            nf_val_loss.append(loss.item)\n",
    "\n",
    "            # plot flow output\n",
    "            zT, _ = flow(Z)\n",
    "            z_0, z_1 = zT[:, 0], zT[:, 1]\n",
    "            k = 2 + plot_epochs.index(epoch+1)\n",
    "            ax[k].scatter(z_0, z_1, alpha=alpha, c=sample_noise_colors, cmap=cmap)\n",
    "            ax[k].set_title(f'Epoch {epoch+1}')\n",
    "\n",
    "# tidy plots\n",
    "for axi in ax:\n",
    "    axi.set_xlim((-4,4))\n",
    "    axi.set_ylim((-4,4))\n",
    "    axi.axes.xaxis.set_visible(False)\n",
    "    axi.axes.yaxis.set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ef0a7",
   "metadata": {},
   "source": [
    "Finally, we compare the shapes produced by different numbers of step sizes.\n",
    "\n",
    "Instead of using scatter plots as above (which is faster), here we plot the resulting distribution using kernel density estimation (KDE).  (The details of KDE are not important; however, it is useful here because it gives some indication of how densely populated the points are in certain regions of space.)\n",
    "\n",
    "We expect to find that more step sizes result in a better fit, with diminishing returns at some point.\n",
    "\n",
    "The first cell below fits the models (fairly quickly); the second cell constructs and displays the KDE plots and is a bit slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "T = [2, 4, 8, 16]\n",
    "# use more epochs this time to get the best possible fit for each model.\n",
    "num_epochs = 250 # decrease to reduce changing time at the cost of a worse fit.\n",
    "for t in T:\n",
    "    print(f't = {t}:')\n",
    "    flow = RealNVPFlow(dim, num_steps=t, conditioner_hidden_dims=conditioner_hidden_dims)\n",
    "    optim = torch.optim.Adam(flow.parameters())\n",
    "\n",
    "    epochs = trange(num_epochs)\n",
    "    for epoch in epochs:\n",
    "        for i in range(num_batches):\n",
    "            x = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            # take step in direction of negative gradient of loss\n",
    "            loss = -flow.loglik(x)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    flows.append(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Density estimation by sampling, using kernel density estimation (KDE)\n",
    "    # This function sometimes misbehaves or gives an error - this is typically\n",
    "    # because one or more of the models trained in the previous step encountered\n",
    "    # numerical problems.  You can skip this cell and view the alternative plots\n",
    "    # below, or try retraining the models in the previous cell.\n",
    "\n",
    "    kde_numpoints = 4000 # Reduce this for quicker, but more granular plots.\n",
    "\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    fig, ax = plt.subplots(1,6, figsize=(18,3), sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].set_title('Data')\n",
    "    X = X_train.numpy()\n",
    "    x_0, x_1 = X[:, 0], X[:, 1]\n",
    "    sns.kdeplot(x=x_0, y=x_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[0])\n",
    "\n",
    "    T = [2,4,8,16]\n",
    "    Z = Normal(\n",
    "            loc=torch.zeros(2, dtype=torch.float32), \n",
    "            scale=torch.ones(2, dtype=torch.float32)\n",
    "        ).sample((kde_numpoints,))\n",
    "    z_0, z_1 = Z[:, 0].numpy(), Z[:, 1].numpy()\n",
    "\n",
    "    sns.kdeplot(x=z_0, y=z_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[1])\n",
    "    ax[1].set_title(r'$p_0=\\mathcal{N}(\\mathbf{0},I)$')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(len(T)):\n",
    "            ax[t+2].set_title(r'$T=${}'.format(T[t]))\n",
    "            model = flows[t]\n",
    "            x, _ = model(Z)\n",
    "            x_0, x_1 = x[:, 0].numpy(), x[:, 1].numpy()\n",
    "            sns.kdeplot(x=x_0, y=x_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[t+2])\n",
    "\n",
    "    for axi in ax:\n",
    "        axi.set_xlim((-4,4))\n",
    "        axi.set_ylim((-4,4))\n",
    "        axi.axes.xaxis.set_visible(False)\n",
    "        axi.axes.yaxis.set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0e6c8",
   "metadata": {},
   "source": [
    "Since we have implemented inversion for this flow, we can do something else to plot the density: we can query the density at various points on a grid, and plot these to get a density surface directly.  This can give us information on the relative density of the flow at places where density is so low that sampling will not give us reliable information.\n",
    "\n",
    "We have followed this approach below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density estimation by querying the density at various points on a grid\n",
    "\n",
    "kde_numpoints = 1000 # Reduce these for quicker, but more granular plots.\n",
    "grid_resolution = 70\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, ax = plt.subplots(1,6, figsize=(18,3), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title('Data')\n",
    "X = X_train.numpy()\n",
    "x_0, x_1 = X[:, 0], X[:, 1]\n",
    "sns.kdeplot(x=x_0, y=x_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[0])\n",
    "\n",
    "T = [2,4,8,16]\n",
    "Z = Normal(\n",
    "        loc=torch.zeros(2, dtype=torch.float32), \n",
    "        scale=torch.ones(2, dtype=torch.float32)\n",
    "    ).sample((kde_numpoints,))\n",
    "z_0, z_1 = Z[:, 0].numpy(), Z[:, 1].numpy()\n",
    "\n",
    "sns.kdeplot(x=z_0, y=z_1, cmap=\"Blues\", shade=True, bw_adjust=.5, ax=ax[1])\n",
    "ax[1].set_title(r'$p_0=\\mathcal{N}(\\mathbf{0},I)$')\n",
    "\n",
    "densities = []\n",
    "with torch.no_grad():\n",
    "    x_0_coords = torch.linspace(-3.0, 3.0, grid_resolution)\n",
    "    x_1_coords = torch.linspace(-3.0, 3.0, grid_resolution)\n",
    "    gm0, gm1 = torch.meshgrid(x_0_coords, x_1_coords)\n",
    "    for t in range(len(T)):\n",
    "        ax[t+2].set_title(r'$T=${}'.format(T[t]))\n",
    "        model = flows[t]\n",
    "        density = torch.zeros((grid_resolution, grid_resolution))\n",
    "\n",
    "        for i in range(grid_resolution):\n",
    "            for j in range(grid_resolution):\n",
    "                density[i,j] = torch.exp(model.loglik(torch.Tensor([[x_0_coords[i], x_1_coords[j]]])))\n",
    "\n",
    "        densities.append(density.detach().numpy())\n",
    "        mappable = ax[t+2].pcolormesh(gm0, gm1, densities[t], shading='auto')\n",
    "\n",
    "for axi in ax:\n",
    "    axi.set_xlim((-4,4))\n",
    "    axi.set_ylim((-4,4))\n",
    "    axi.axes.xaxis.set_visible(False)\n",
    "    axi.axes.yaxis.set_visible(False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0468b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
